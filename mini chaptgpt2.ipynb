{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19279cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, n_head_dim, dropout):\n",
    "        super().__init__()\n",
    "        # attention dimensions:\n",
    "        # - key and query dimensions need to be the same\n",
    "        # - value dimension can be different, but in practice, we set it to the same thing\n",
    "        self.n_kq = n_head_dim\n",
    "        self.n_v = n_head_dim\n",
    "        self.n_kq_tot = n_head * self.n_kq\n",
    "        self.n_v_tot = n_head * self.n_v\n",
    "        \n",
    "        # projections from embedding dim to total attention dim (split into q, k, v) and back\n",
    "        self.qkv = nn.Linear(n_embd, 2 * self.n_kq_tot + self.n_v_tot)\n",
    "        self.proj = nn.Linear(self.n_v_tot, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.n_head_dim = n_head_dim\n",
    "        self.scale = self.n_kq ** -0.5\n",
    "\n",
    "    def _split_heads(self, tensor, batch_size, seq_len, n_head, n_head_dim):\n",
    "        \"\"\"Reshape tensor from (B, T, n_head * n_head_dim) into (B, n_head, T, n_head_dim)\"\"\"\n",
    "        return tensor.view(batch_size, seq_len, n_head, n_head_dim).transpose(1, 2)\n",
    "    \n",
    "    def _merge_heads(self, tensor, batch_size, seq_len, n_head, n_head_dim):\n",
    "        \"\"\"Reshape tensor from (B, n_head, T, n_head_dim) into (B, T, n_head * n_head_dim)\"\"\"\n",
    "        return tensor.transpose(1, 2).contiguous().view(batch_size, seq_len, n_head * n_head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # project all qkv matrices at once, then split\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.split([self.n_kq_tot, self.n_kq_tot, self.n_v_tot], dim=-1)\n",
    "        \n",
    "        # reshape each of q, k, v from (B, T, n_head * n_head_dim) into (B, n_head, T, n_head_dim)\n",
    "        q = self._split_heads(q, B, T, self.n_head, self.n_kq)\n",
    "        k = self._split_heads(k, B, T, self.n_head, self.n_kq)\n",
    "        v = self._split_heads(v, B, T, self.n_head, self.n_v)\n",
    "\n",
    "        # compute attention scores\n",
    "        # 1. combine q and k to make a square matrix for query-key matching (relevance LUT)\n",
    "        # 2. scale to 1/âˆšn_kq, because otherwise the dot products grow with the number of dimensions\n",
    "        # 3. causal masking\n",
    "        # 4. scale relevance scores to add up to 1, so we can effectively add them up\n",
    "        # 5. get the weighted contributions of all value embeddings according to their relevance\n",
    "        att = (q @ k.transpose(-2, -1))\n",
    "        att = att * self.scale\n",
    "        att = att.masked_fill(torch.tril(torch.ones(T, T, device=att.device)) == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        y = att @ v\n",
    "\n",
    "        # reshape output from (B, n_head, T, n_head_dim) into (B, T, n_head * n_head_dim)\n",
    "        y = self._merge_heads(y, B, T, self.n_head, self.n_v)\n",
    "        # project from attention embeddings back to token embeddings\n",
    "        y = self.proj(y)\n",
    "        return self.dropout(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522db69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        # All operations maintain the batch and sequence length dimensions,\n",
    "        # only transforming the embedding dimension.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a0acc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, n_head_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, n_head_dim, dropout)\n",
    "        self.ln_2 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = MLP(n_embd, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x+ is the residual connection\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe068427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7245f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your text dataset\n",
    "DATASET_PATH = \"data/tiny-shakespeare.txt\"  # Relative path for dataset\n",
    "dataset = load_dataset('text', data_files=DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4920da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./mini_chatgpt',  # Saves to the same folder as your notebook\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',  # Logs in the same folder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "import os\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define a function to preprocess the Shakespeare text file\n",
    "def prepare_dataset(file_path, tokenizer, block_size=128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "# Prepare the dataset\n",
    "dataset = prepare_dataset(DATASET_PATH, tokenizer)\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "output_dir = \"models/final_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure directory exists\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"logs\",  # Relative path for logs\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "MODEL_DIR = \"./models/final_model\"  # Relative path for portability\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)  # Ensure directory exists\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "\n",
    "# Test the trained model\n",
    "def generate_text(prompt, model, tokenizer, max_length=50):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model with a prompt\n",
    "prompt = \"O Romeo, Romeo, wherefore art thou Romeo?\"\n",
    "generated_text = generate_text(prompt, model, tokenizer)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define relative path for saving the model\n",
    "MODEL_DIR = \"./models/final_model\"\n",
    "\n",
    "# Ensure the directory exists before saving\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Define relative path for the checkpoint\n",
    "MODEL_DIR = \"./models/final_model\"\n",
    "\n",
    "# Ensure the model directory exists before loading\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    raise FileNotFoundError(f\"Model checkpoint not found at {MODEL_DIR}. Make sure to train or place the model in this directory.\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
